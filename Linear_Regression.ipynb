{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression\n",
    "Linear regression aims to model the linear relationship between a dependent variable $y$ and independent variables $\\boldsymbol{x}$ by a function $f$. Suppose we want to predict the price of a house $y$ based on several features such as the size of the house in square feet $x_1$, and the number of bedrooms $x_2$.\n",
    "<br>\n",
    "\\begin{array}{|c|c|c|c|}\n",
    "\\hline\n",
    "\\rule{0pt}{3ex}\\text{House ID} & \\text{Size}\\,(x_1) & \\text{Bedrooms}\\,(x_2) & \\text{Price}\\,(y)\\rule[-1.25ex]{0pt}{0pt} \\\\\n",
    "\\hline\n",
    "1 & \\rule{0pt}{3ex}2104\\rule[-1.0ex]{0pt}{0pt} & \\rule{0pt}{3ex}3\\rule[-1.0ex]{0pt}{0pt} & \\rule{0pt}{3ex}399900\\rule[-1.0ex]{0pt}{0pt} \\\\\n",
    "\\hline\n",
    "2 & \\rule{0pt}{3ex}1600\\rule[-1.0ex]{0pt}{0pt} & \\rule{0pt}{3ex}3\\rule[-1.0ex]{0pt}{0pt} & \\rule{0pt}{3ex}329900\\rule[-1.0ex]{0pt}{0pt} \\\\\n",
    "\\hline\n",
    "3 & \\rule{0pt}{3ex}2400\\rule[-1.0ex]{0pt}{0pt} & \\rule{0pt}{3ex}3\\rule[-1.0ex]{0pt}{0pt} & \\rule{0pt}{3ex}369000\\rule[-1.0ex]{0pt}{0pt} \\\\\n",
    "\\hline\n",
    "4 & \\rule{0pt}{3ex}1416\\rule[-1.0ex]{0pt}{0pt} & \\rule{0pt}{3ex}2\\rule[-1.0ex]{0pt}{0pt} & \\rule{0pt}{3ex}232000\\rule[-1.0ex]{0pt}{0pt} \\\\\n",
    "\\hline\n",
    "5 & \\rule{0pt}{3ex}3000\\rule[-1.0ex]{0pt}{0pt} & \\rule{0pt}{3ex}4\\rule[-1.0ex]{0pt}{0pt} & \\rule{0pt}{3ex}539900\\rule[-1.0ex]{0pt}{0pt} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "<br>\n",
    "The dependent variable is $y$, and the independent variables are $(x_1, x_2)$. The linear regression model then can be written as:\n",
    "$$y = f(\\boldsymbol{x}) = \\boldsymbol{\\beta}^\\top \\boldsymbol{x} + \\epsilon$$\n",
    "$\\boldsymbol{x}$ is a vector including a constant term for the intercept:\n",
    "$$\\boldsymbol{x} = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\end{bmatrix}$$\n",
    "and $\\boldsymbol{\\beta}$ is the vector of **parameters** (or **weights**):\n",
    "$$\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\beta_2 \\\\ \\end{bmatrix}$$\n",
    "The parameters quantify the relationship between the dependent variable ($y$) and the independent variables ($\\boldsymbol{x}$). In our house price prediction example, these parameters help estimate how size, number of bedrooms, and age influence the overall price of a house. \n",
    "\n",
    "In the context of linear regression, the **intercept** is a parameter in the regression model that represents the expected value of the dependent variable $y$ when all the independent variables $(x_1, x_2)$ are equal to zero. The intercept $\\beta_0$ is crucial because it allows the model to fit the data more accurately by adjusting the baseline level of $y$. Without the intercept, the regression line would be forced to pass through the origin (i.e., $y = 0$ when $x_1 = 0$, and $x_2 = 0$), which may not be appropriate for many datasets.\n",
    "\n",
    "Moreover, $\\epsilon$ is the **error term**. It represents the difference between the observed values of the dependent variable $y$ and the values predicted by the linear regression model. It accounts for the variability in $y$ that cannot be explained by the linear relationship with the independent variables $\\boldsymbol{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Size</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2104</td>\n",
       "      <td>3</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1416</td>\n",
       "      <td>2</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000</td>\n",
       "      <td>4</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Size  Bedrooms   Price\n",
       "0  2104         3  399900\n",
       "1  1600         3  329900\n",
       "2  2400         3  369000\n",
       "3  1416         2  232000\n",
       "4  3000         4  539900"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Prepare the data\n",
    "import pandas as pd\n",
    "data = pd.read_csv('house.txt', header=None, names=['Size', 'Bedrooms', 'Price'])\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intercept</th>\n",
       "      <th>Size</th>\n",
       "      <th>Bedrooms</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2104</td>\n",
       "      <td>3</td>\n",
       "      <td>399900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1600</td>\n",
       "      <td>3</td>\n",
       "      <td>329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2400</td>\n",
       "      <td>3</td>\n",
       "      <td>369000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1416</td>\n",
       "      <td>2</td>\n",
       "      <td>232000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3000</td>\n",
       "      <td>4</td>\n",
       "      <td>539900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Intercept  Size  Bedrooms   Price\n",
       "0          1  2104         3  399900\n",
       "1          1  1600         3  329900\n",
       "2          1  2400         3  369000\n",
       "3          1  1416         2  232000\n",
       "4          1  3000         4  539900"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Add intercept term\n",
    "data.insert(0, 'Intercept', 1)  \n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepare X and y\n",
    "X = data[['Intercept', 'Size', 'Bedrooms']].values\n",
    "y = data['Price'].values\n",
    "y = y.reshape(-1, 1)  # reshapes y into a column vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function\n",
    "To determined how well the linear regression model fits the data, the cost function is applied. The most commonly used cost function is the **Mean Squared Error (MSE)**, which quantifies the difference between the predicted values and the actual values of the dependent variable. The MSE can be defined as:\n",
    "\\begin{align}\n",
    "J(\\boldsymbol{\\beta}) &= \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - \\hat{y}_i \\right)^2\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n \\left(y_i - f({x}_{i}) \\right)^2\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n \\left[y_i -  (\\boldsymbol{\\beta}^\\top \\boldsymbol{x}^{(i)} + \\epsilon)\\right]^2\\\\\n",
    "\\end{align}\n",
    "where $n$ is the number of observations (data points), $\\boldsymbol{x}^{(i)}$ are the independent variables for the $i$-th observation, and $\\hat{y}_i$ is the predicted value of the dependent variable for the $i$-th observation. We seek to make $f({x}_{i})$ close to $y_i$. Thus, the goal of the linear regression algorithm is to find the parameters $\\boldsymbol{\\beta}$ that minimize this cost function. To achieve this, we can use the **LMS algorithm**.\n",
    "\n",
    "## LMS Algorithm\n",
    "The **LMS (Least Mean Square) update rule**, also known as the **Widrow-Hoff learning rule**, updates the parameters iteratively to minimize the cost function The update rule for LMS is:\n",
    "$$\\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta} - \\alpha \\nabla J(\\boldsymbol{\\beta})$$\n",
    "The learning rate, represented by $\\alpha$, is a positive scalar determining the step size for each iteration, and $\\nabla J(\\boldsymbol{\\beta})$ is the gradient of the cost function with respect to $\\boldsymbol{\\beta}$. Since $\\epsilon$ are already reflected in the residuals $(y_i - \\hat{y}_i)$, we do not need to explicitly include $\\epsilon$ in the cost function. By equation (3), consider the gradient with respect to a single parameter $\\beta_j$:\n",
    "\\begin{align*}\n",
    "\\frac{\\partial J(\\boldsymbol{\\beta})}{\\partial \\beta_j} &= \\frac{1}{n} \\sum_{i=1}^n \\frac{\\partial}{\\partial \\beta_j} \\left( y_i - \\boldsymbol{\\beta}^\\top \\boldsymbol{x}^{(i)} \\right)^2\\\\\n",
    "&= \\frac{1}{n} \\sum_{i=1}^n 2 \\left( y_i - \\boldsymbol{\\beta}^\\top \\boldsymbol{x}^{(i)}\\right) \\cdot \\left( -\\frac{\\partial}{\\partial \\beta_j} (\\boldsymbol{\\beta}^\\top \\boldsymbol{x}^{(i)}) \\right)\\\\\n",
    "&= -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\boldsymbol{\\beta}^\\top \\boldsymbol{x}^{(i)}) x^{(i)}_j\n",
    "\\end{align*}\n",
    "Therefore, the gradient vector can be written as:\n",
    "$$\\nabla J(\\boldsymbol{\\beta}) = -\\frac{2}{n} \\sum_{i=1}^n (y_i - \\boldsymbol{\\beta}^\\top \\boldsymbol{x}^{(i)}) \\boldsymbol{x}^{(i)}$$\n",
    "To facilitate the derivation and computation of the gradient, the cost function can be modified to include a factor of $\\frac{1}{2}$:\n",
    "\\begin{align*}\n",
    "J(\\boldsymbol{\\beta}) = \\frac{1}{2n} \\sum_{i=1}^n \\left[y_i -  (\\boldsymbol{\\beta}^\\top \\boldsymbol{x}^{(i)} + \\epsilon)\\right]^2\n",
    "\\end{align*}\n",
    "Thus, the gradient of the cost function with respect to the parameters $\\boldsymbol{\\beta}$ becomes:\n",
    "$$\\nabla J(\\boldsymbol{\\beta}) = -\\frac{1}{n} \\sum_{i=1}^n (y_i - \\boldsymbol{\\beta}^\\top \\boldsymbol{x}^{(i)}) \\boldsymbol{x}^{(i)}$$\n",
    "which makes the derivative cleaner and easier to work with.\n",
    "\n",
    "\n",
    "In matrix notation, we can represent the gradient computation more compactly. Let $\\mathbf{X}$ be the matrix of input features, where each row is $\\boldsymbol{x}^{(i)}$. Futhermore, $\\boldsymbol{y}$ is the $n \\times 1$ vector of target values. Thus, the cost function can be represented as:\n",
    "$$J(\\boldsymbol{\\beta}) = \\frac{1}{2n} (\\mathbf{X} \\boldsymbol{\\beta} - \\boldsymbol{y})^\\top (\\mathbf{X} \\boldsymbol{\\beta} - \\boldsymbol{y})$$\n",
    "The gradient of the cost function can be computed as:\n",
    "\\begin{align*}\n",
    "\\nabla_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta}) \n",
    "&= \\nabla_{\\boldsymbol{\\beta}} \\left(\\frac{1}{2n} (\\mathbf{X} \\boldsymbol{\\beta} - \\boldsymbol{y})^\\top (\\mathbf{X} \\boldsymbol{\\beta} - \\boldsymbol{y}) \\right)\\\\\n",
    "&= \\nabla_{\\boldsymbol{\\beta}} \\left[\\frac{1}{2n} \\left((\\mathbf{X} \\boldsymbol{\\beta})^\\top \\mathbf{X} \\boldsymbol{\\beta} - (\\mathbf{X} \\boldsymbol{\\beta})^\\top \\boldsymbol{y} - \\boldsymbol{y}^\\top \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{y}^\\top \\boldsymbol{y} \\right) \\right]\\\\\n",
    "&= \\nabla_{\\boldsymbol{\\beta}} \\left( \\frac{1}{2n} \\left( \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} - \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\boldsymbol{y} - \\boldsymbol{y}^\\top \\mathbf{X} \\boldsymbol{\\beta} + \\boldsymbol{y}^\\top \\boldsymbol{y} \\right) \\right)\\\\\n",
    "\\end{align*}\n",
    "Note that \n",
    "$${((\\mathbf{X} \\boldsymbol{\\beta})^\\top \\boldsymbol{y})}_{1\\times 1} = (\\boldsymbol{y}^\\top (\\mathbf{X} \\boldsymbol{\\beta}))^\\top = \\boldsymbol{y}^\\top \\mathbf{X} \\boldsymbol{\\beta}$$\n",
    "Thus, the expression can be simplified to \n",
    "\\begin{align*}\n",
    "\\nabla_{\\boldsymbol{\\beta}} J(\\boldsymbol{\\beta}) \n",
    "&= \\nabla_{\\boldsymbol{\\beta}} \\left( \\frac{1}{2n} \\left( \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} - 2 \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\boldsymbol{y} + \\boldsymbol{y}^\\top \\boldsymbol{y} \\right) \\right) \\\\\n",
    "&= \\frac{1}{2n} \\left( \\nabla_{\\boldsymbol{\\beta}} \\left( \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} \\right) - 2 \\nabla_{\\boldsymbol{\\beta}} \\left( \\boldsymbol{\\beta}^\\top \\mathbf{X}^\\top \\boldsymbol{y} \\right) + \\nabla_{\\boldsymbol{\\beta}} \\left( \\boldsymbol{y}^\\top \\boldsymbol{y} \\right) \\right)\\\\\n",
    "&= \\frac{1}{2n} \\left( 2 \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} - 2 \\mathbf{X}^\\top \\boldsymbol{y} + 0 \\right)\\\\\n",
    "&= \\frac{2}{2n} \\left( \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} - \\mathbf{X}^\\top \\boldsymbol{y} \\right)\\\\\n",
    "&= \\frac{1}{n} \\left( \\mathbf{X}^\\top \\mathbf{X} \\boldsymbol{\\beta} - \\mathbf{X}^\\top \\boldsymbol{y} \\right)\\\\\n",
    "&= \\frac{1}{n}\\,\\,\\mathbf{X}^\\top\\left(\\mathbf{X} \\boldsymbol{\\beta} - \\boldsymbol{y}\\right)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cost function\n",
    "import numpy as np\n",
    "def cost(X, y, beta):\n",
    "    n = len(y)\n",
    "    J = (1 / (2 * n)) * np.sum((X @ beta - y) ** 2)\n",
    "    return J\n",
    "\n",
    "## Gradient \n",
    "def gradient(X, y, beta):\n",
    "    n = len(y)\n",
    "    gradient = (1 / n) * (X.T @ (X @ beta - y))\n",
    "    return gradient\n",
    "\n",
    "## Update rule\n",
    "def update(beta, gradient, alpha):\n",
    "    beta_new = beta - alpha * gradient\n",
    "    return beta_new\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent\n",
    "The **gradient descent** is an optimization algorithm used to minimize the cost function in a linear regression model. It follows these steps:\n",
    "1. **Initialize** the parameters $\\boldsymbol{\\beta}$ with some initial values (e.g., zeros or small random numbers).\n",
    "\n",
    "2. **Repeat** until convergence:\n",
    "\n",
    "   1. **Calculate the gradient** $\\nabla J(\\boldsymbol{\\beta})$ of the cost function with respect to the parameters.\n",
    "\n",
    "   2. **Update the parameters** using the gradient and the learning rate $\\alpha$:\n",
    "      $$\\boldsymbol{\\beta} \\leftarrow \\boldsymbol{\\beta} - \\alpha \\nabla J(\\boldsymbol{\\beta})$$\n",
    "      \n",
    "3. **Check for convergence**:\n",
    "   - If the change in the cost function is smaller than a predefined threshold, or the maximum number of iterations is reached, stop.\n",
    "\n",
    "For linear regression, the cost function is convex with a single global minimum. This ensures that gradient descent will converge to the global minimum, provided the learning rate is appropriately chosenâ€‹. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, beta, alpha, num_iters):\n",
    "    cost_history = np.zeros(num_iters)\n",
    "    for i in range(num_iters):\n",
    "        grad = gradient(X, y, beta)\n",
    "        beta = update(beta, grad, alpha)\n",
    "        cost_history[i] = cost(X, y, beta)\n",
    "        if i > 0 and abs(cost_history[i] - cost_history[i-1]) < 1e-6:\n",
    "            break\n",
    "    return beta, cost_history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialization of Gradient Descent\n",
    "The parameters (or weights) of the linear regression model, represented by $\\boldsymbol{\\beta}$, need to be initialized before the gradient descent algorithm begins. Common strategies for initializing these parameters include:\n",
    "\n",
    "##### 1. Zero Initialization\n",
    "This method sets all parameters to zero:\n",
    "$$\\boldsymbol{\\beta} = \\begin{bmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix}$$\n",
    "This method is simple and safe for linear regression, but can lead to slow convergence in more complex models.\n",
    "\n",
    "##### 2. All-One Initialization\n",
    "This method sets all parameters to one:\n",
    "$$\\boldsymbol{\\beta} = \\begin{bmatrix} 1 \\\\ 1 \\\\ \\vdots \\\\ 1 \\end{bmatrix}$$\n",
    "This method is easy to implement and straightforward, but may lead to large initial gradients, causing slow or unstable convergence if features are not normalized.\n",
    "\n",
    "##### 3. Random Initialization\n",
    "Parameters are initialized with small random values:\n",
    "$$\\boldsymbol{\\beta} = \\begin{bmatrix} \\beta_0 \\\\ \\beta_1 \\\\ \\vdots \\\\ \\beta_k \\end{bmatrix}$$\n",
    "where $\\beta_i$ are small random numbers typically drawn from a uniform or normal distribution.\n",
    "This method promotes varied starting points for potentially faster convergence, but needs careful selection to avoid instability from large initial gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init(n_features, init_type='zeros'):\n",
    "    if init_type == 'zeros':\n",
    "        beta = np.zeros((n_features, 1))\n",
    "    elif init_type == 'ones':\n",
    "        beta = np.ones((n_features, 1))\n",
    "    elif init_type == 'random':\n",
    "        beta = np.random.randn(n_features, 1) * 0.01\n",
    "    return beta\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
